relationships:
  success: {en: Flow Files that have been successfully processed are transferred to
      this relationship, zh: 已成功处理的流文件将传输到此关系}
  failure: {en: Flow Files that could not be processed due to issue that cannot be
      retried are transferred to this relationship, zh: 由于无法重试的问题而无法处理的流文件将传输到此关系}
  retry: {en: Flow Files that could not be processed due to issues that can be retried
      are transferred to this relationship, zh: 由于可以重试的问题而无法处理的流文件将传输到此关系}
capabilityDescription: {en: 'Reads records from an incoming FlowFile using the provided
    Record Reader, and writes those records to a Parquet file. The schema for the
    Parquet file must be provided in the processor properties. This processor will
    first write a temporary dot file and upon successfully writing every record to
    the dot file, it will rename the dot file to it''s final name. If the dot file
    cannot be renamed, the rename operation will be attempted up to 10 times, and
    if still not successful, the dot file will be deleted and the flow file will be
    routed to failure.  If any error occurs while reading records from the input,
    or writing records to the output, the entire dot file will be removed and the
    flow file will be routed to failure or retry, depending on the error.', zh: 使用提供的记录读取器从传入的FlowFile读取记录，并将这些记录写入Parquet文件。必须在处理器属性中提供Parquet文件的架构。该处理器将首先写入一个临时点文件，并在成功将每条记录写入点文件后，将点文件重命名为其最终名称。如果无法重命名点文件，将尝试最多10次重命名操作，如果仍不成功，将删除点文件并将流文件路由到失败。如果在从输入读取记录或将记录写入输出时发生任何错误，将删除整个点文件，并根据错误将流文件路由到失败或重试。}
readAttributes:
  filename: {en: The name of the file to write comes from the value of this attribute.,
    zh: 要写入的文件的名称来自此属性的值。}
properties:
  Hadoop Configuration Resources:
    en: {displayName: Hadoop Configuration Resources, description: 'A file or comma
        separated list of files which contains the Hadoop file system configuration.
        Without this, Hadoop will search the classpath for a ''core-site.xml'' and
        ''hdfs-site.xml'' file or will revert to a default configuration. To use swebhdfs,
        see ''Additional Details'' section of PutHDFS''s documentation.'}
    zh: {description: 包含Hadoop文件系统配置的文件或逗号分隔的文件列表。否则，Hadoop将在类路径中搜索“核心站点”。xml'和'hdfs站点。xml文件，或将恢复为默认配置。要使用swebhdfs，请参阅PutHDFS文档的“其他详细信息”部分。,
      displayName: Hadoop配置资源}
  max-padding-size:
    en: {displayName: Max Padding Size, description: 'The maximum amount of padding
        that will be used to align row groups with blocks in the underlying filesystem.
        If the underlying filesystem is not a block filesystem like HDFS, this has
        no effect. The value is specified in the format of <Data Size> <Data Unit>
        where Data Unit is one of B, KB, MB, GB, TB.'}
    zh: {description: 用于将行组与底层文件系统中的块对齐的最大填充量。如果底层文件系统不是像HDFS那样的块文件系统，那么这不会产生任何影响。该值以＜数据大小＞＜数据单元＞的格式指定，其中数据单元是B、KB、MB、GB、TB之一。,
      displayName: 最大衬垫尺寸}
  Additional Classpath Resources:
    en: {displayName: Additional Classpath Resources, description: 'A comma-separated
        list of paths to files and/or directories that will be added to the classpath
        and used for loading native libraries. When specifying a directory, all files
        with in the directory will be added to the classpath, but further sub-directories
        will not be included.'}
    zh: {description: 以逗号分隔的文件和/或目录路径列表，这些路径将添加到类路径中并用于加载本机库。指定目录时，目录中的所有文件都将添加到类路径中，但不包括其他子目录。,
      displayName: 其他Classpath资源}
  kerberos-credentials-service:
    en: {displayName: Kerberos Credentials Service, description: Specifies the Kerberos
        Credentials Controller Service that should be used for authenticating with
        Kerberos}
    zh: {description: 指定应用于Kerberos身份验证的Kerberos凭据控制器服务, displayName: Kerberos凭据服务}
  page-size:
    en: {displayName: Page Size, description: 'The page size used by the Parquet writer.
        The value is specified in the format of <Data Size> <Data Unit> where Data
        Unit is one of B, KB, MB, GB, TB.'}
    zh: {description: Parquet编写器使用的页面大小。该值以＜数据大小＞＜数据单元＞的格式指定，其中数据单元是B、KB、MB、GB、TB之一。,
      displayName: 页面大小}
  writer-version:
    en: {displayName: Writer Version, description: Specifies the version used by Parquet
        writer}
    zh: {description: 指定Parquet编写器使用的版本, displayName: Writer版本}
  Kerberos Password:
    en: {displayName: Kerberos Password, description: Kerberos password associated
        with the principal.}
    zh: {description: 与主体关联的Kerberos密码。, displayName: Kerberos密码}
  Kerberos Keytab:
    en: {displayName: Kerberos Keytab, description: Kerberos keytab associated with
        the principal. Requires nifi.kerberos.krb5.file to be set in your nifi.properties}
    zh: {description: 与主体关联的Kerberos密钥表。需要在nifi.properties中设置nifi.krberos.krb5.file,
      displayName: Kerberos密钥选项卡}
  Kerberos Principal:
    en: {displayName: Kerberos Principal, description: Kerberos principal to authenticate
        as. Requires nifi.kerberos.krb5.file to be set in your nifi.properties}
    zh: {description: 要作为身份验证的Kerberos主体。需要在nifi.properties中设置nifi.krberos.krb5.file,
      displayName: Kerberos主体}
  kerberos-user-service:
    en: {displayName: Kerberos User Service, description: Specifies the Kerberos User
        Controller Service that should be used for authenticating with Kerberos}
    zh: {description: 指定应用于Kerberos身份验证的Kerberos用户控制器服务, displayName: Kerberos用户服务}
  record-reader:
    en: {displayName: Record Reader, description: The service for reading records
        from incoming flow files.}
    zh: {description: 用于从传入流文件读取记录的服务。, displayName: 记录读取器}
  overwrite:
    en: {displayName: Overwrite Files, description: 'Whether or not to overwrite existing
        files in the same directory with the same name. When set to false, flow files
        will be routed to failure when a file exists in the same directory with the
        same name.'}
    zh: {description: 是否用相同的名称覆盖同一目录中的现有文件。当设置为false时，当同一目录中存在同名文件时，流文件将被路由到失败。, displayName: 覆盖文件}
  avro-add-list-element-records:
    en: {displayName: Avro Add List Element Records, description: Specifies the value
        for 'parquet.avro.add-list-element-records' in the underlying Parquet library}
    zh: {description: 指定'parquet.avro的值。在基础Parquet库中添加列表元素记录, displayName: Avro添加列表元素记录}
  remove-crc-files:
    en: {displayName: Remove CRC Files, description: Specifies whether the corresponding
        CRC file should be deleted upon successfully writing a Parquet file}
    zh: {description: 指定在成功写入Parquet文件时是否应删除相应的CRC文件, displayName: 删除CRC文件}
  Directory:
    en: {displayName: Directory, description: The parent directory to which files
        should be written. Will be created if it doesn't exist.}
    zh: {description: 文件应写入的父目录。如果不存在，将创建。, displayName: 目录}
  avro-write-old-list-structure:
    en: {displayName: Avro Write Old List Structure, description: Specifies the value
        for 'parquet.avro.write-old-list-structure' in the underlying Parquet library}
    zh: {description: 指定'parquet.avro的值。在底层Parquet库中写入旧列表结构, displayName: Avro写入旧列表结构}
  permissions-umask:
    en: {displayName: Permissions umask, description: A umask represented as an octal
        number which determines the permissions of files written to HDFS. This overrides
        the Hadoop Configuration dfs.umaskmode}
    zh: {description: 表示为八进制数的umask，用于确定写入HDFS的文件的权限。这将覆盖Hadoop配置dfs.umaskmode, displayName: 权限umask}
  remote-group:
    en: {displayName: Remote Group, description: Changes the group of the HDFS file
        to this value after it is written. This only works if NiFi is running as a
        user that has HDFS super user privilege to change group}
    zh: {description: 写入HDFS文件后，将其组更改为该值。只有当NiFi以具有HDFS超级用户权限的用户身份运行时，才能更改组, displayName: 远程组}
  row-group-size:
    en: {displayName: Row Group Size, description: 'The row group size used by the
        Parquet writer. The value is specified in the format of <Data Size> <Data
        Unit> where Data Unit is one of B, KB, MB, GB, TB.'}
    zh: {description: Parquet编写器使用的行组大小。该值以＜数据大小＞＜数据单元＞的格式指定，其中数据单元是B、KB、MB、GB、TB之一。,
      displayName: 行组大小}
  remote-owner:
    en: {displayName: Remote Owner, description: Changes the owner of the HDFS file
        to this value after it is written. This only works if NiFi is running as a
        user that has HDFS super user privilege to change owner}
    zh: {description: 写入HDFS文件后，将其所有者更改为该值。只有当NiFi以具有HDFS超级用户权限的用户身份运行时，才能更改所有者, displayName: 远程所有者}
  enable-dictionary-encoding:
    en: {displayName: Enable Dictionary Encoding, description: Specifies whether dictionary
        encoding should be enabled for the Parquet writer}
    zh: {description: 指定是否应为Parquet编写器启用字典编码, displayName: 启用字典编码}
  compression-type:
    en: {displayName: Compression Type, description: The type of compression for the
        file being written.}
    zh: {description: 正在写入的文件的压缩类型。, displayName: 压缩类型}
  enable-validation:
    en: {displayName: Enable Validation, description: Specifies whether validation
        should be enabled for the Parquet writer}
    zh: {description: 指定是否应为Parquet编写器启用验证, displayName: 启用验证}
  dictionary-page-size:
    en: {displayName: Dictionary Page Size, description: 'The dictionary page size
        used by the Parquet writer. The value is specified in the format of <Data
        Size> <Data Unit> where Data Unit is one of B, KB, MB, GB, TB.'}
    zh: {description: Parquet作者使用的字典页面大小。该值以＜数据大小＞＜数据单元＞的格式指定，其中数据单元是B、KB、MB、GB、TB之一。,
      displayName: 字典页面大小}
  Kerberos Relogin Period:
    en:
      displayName: Kerberos Relogin Period
      description: |-
        Period of time which should pass before attempting a kerberos relogin.

        This property has been deprecated, and has no effect on processing. Relogins now occur automatically.
    zh: {description: 在尝试kerberos重新登录之前应该经过的时间段。, displayName: Kerberos时钟周期}
writeAttributes:
  filename: {en: The name of the file is stored in this attribute., zh: 文件名存储在此属性中。}
  record.count: {en: The number of records written to the Parquet file, zh: 写入Parquet文件的记录数}
  absolute.hdfs.path: {en: The absolute path to the file is stored in this attribute.,
    zh: 文件的绝对路径存储在此属性中。}
tags:
  en: [put, parquet, hadoop, HDFS, filesystem, record]
  zh: [放, 拼花地板, hadoop公司, 高密度光纤, 文件系统, 记录]
