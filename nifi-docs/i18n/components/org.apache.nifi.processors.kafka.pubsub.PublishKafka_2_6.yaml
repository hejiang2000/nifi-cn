relationships:
  success: {en: FlowFiles for which all content was sent to Kafka., zh: 所有内容都发送到Kafka的FlowFiles。}
  failure: {en: Any FlowFile that cannot be sent to Kafka will be routed to this Relationship,
    zh: 无法发送到Kafka的任何FlowFile都将被路由到此关系}
dynamicProperties:
  The name of a Kafka configuration property.:
    en: {description: 'These properties will be added on the Kafka configuration after
        loading any provided configuration properties. In the event a dynamic property
        represents a property that was already set, its value will be ignored and
        WARN message logged. For the list of available Kafka properties please refer
        to: http://kafka.apache.org/documentation.html#configuration. ', value: The
        value of a given Kafka configuration property.}
    zh: {description: '这些属性将在加载任何提供的配置属性后添加到Kafka配置中。如果动态属性表示已设置的属性，则将忽略其值并记录WARN消息。有关可用Kafka属性的列表，请参阅：http://kafka.apache.org/documentation.html#configuration.',
      value: 给定Kafka配置属性的值。}
capabilityDescription: {en: 'Sends the contents of a FlowFile as a message to Apache
    Kafka using the Kafka 2.6 Producer API.The messages to send may be individual
    FlowFiles or may be delimited, using a user-specified delimiter, such as a new-line.
    The complementary NiFi processor for fetching messages is ConsumeKafka_2_6.',
  zh: 使用Kafka 2.6 Producer API将FlowFile的内容作为消息发送给Apache Kafka。要发送的消息可以是单独的FlowFiles，也可以使用用户指定的分隔符（如新行）进行分隔。用于获取消息的补充NiFi处理器是ConsumerKafka_2_6。}
properties:
  compression.type:
    en: {displayName: Compression Type, description: This parameter allows you to
        specify the compression codec for all data generated by this producer.}
    zh: {description: 此参数允许您为此生成器生成的所有数据指定压缩编解码器。, displayName: 压缩类型}
  attribute-name-regex:
    en: {displayName: Attributes to Send as Headers (Regex), description: 'A Regular
        Expression that is matched against all FlowFile attribute names. Any attribute
        whose name matches the regex will be added to the Kafka messages as a Header.
        If not specified, no FlowFile attributes will be added as headers.'}
    zh: {description: 与所有FlowFile属性名称匹配的正则表达式。任何名称与正则表达式匹配的属性都将作为Header添加到Kafka消息中。如果未指定，则不会将FlowFile属性添加为标头。,
      displayName: 要作为标头发送的属性（Regex）}
  bootstrap.servers:
    en: {displayName: Kafka Brokers, description: 'Comma-separated list of Kafka Brokers
        in the format host:port'}
    zh: {description: 'Kafka Broker的逗号分隔列表，格式为host:port', displayName: 卡夫卡经纪人}
  sasl.kerberos.principal:
    en: {displayName: Kerberos Principal, description: Principal used for authentication
        with Kerberos}
    zh: {description: 用于Kerberos身份验证的主体, displayName: Kerberos主体}
  sasl.kerberos.service.name:
    en: {displayName: Kerberos Service Name, description: The service name that matches
        the primary name of the Kafka server configured in the broker JAAS configuration}
    zh: {description: 与代理JAAS配置中配置的Kafka服务器的主名称匹配的服务名称, displayName: Kerberos服务名称}
  kerberos-credentials-service:
    en: {displayName: Kerberos Credentials Service, description: Service supporting
        generalized credentials authentication with Kerberos}
    zh: {description: 支持Kerberos通用凭据身份验证的服务, displayName: Kerberos凭据服务}
  kafka-key:
    en: {displayName: Kafka Key, description: 'The Key to use for the Message. If
        not specified, the flow file attribute ''kafka.key'' is used as the message
        key, if it is present.Beware that setting Kafka key and demarcating at the
        same time may potentially lead to many Kafka messages with the same key.Normally
        this is not a problem as Kafka does not enforce or assume message and key
        uniqueness. Still, setting the demarcator and Kafka key at the same time poses
        a risk of data loss on Kafka. During a topic compaction on Kafka, messages
        will be deduplicated based on this key.'}
    zh: {description: 用于消息的密钥。如果未指定，则流文件属性“kafka”。key”用作消息密钥（如果存在）。请注意，同时设置Kafka密钥和划分可能会导致许多Kafka消息使用相同的密钥。通常，这不是一个问题，因为Kafka不强制或假设消息和密钥的唯一性。尽管如此，同时设置分界符和Kafka密钥会给Kafka带来数据丢失的风险。在Kafka上进行主题压缩时，消息将基于此密钥进行重复数据消除。,
      displayName: 卡夫卡密钥}
  partition:
    en: {displayName: Partition, description: Specifies which Partition Records will
        go to.}
    zh: {description: 指定将转到哪个分区记录。, displayName: 隔断}
  sasl.mechanism:
    en: {displayName: SASL Mechanism, description: SASL mechanism used for authentication.
        Corresponds to Kafka Client sasl.mechanism property}
    zh: {description: 用于身份验证的SASL机制。对应Kafka Client sasl。机械特性, displayName: SASL机制}
  max.block.ms:
    en: {displayName: Max Metadata Wait Time, description: The amount of time publisher
        will wait to obtain metadata or wait for the buffer to flush during the 'send'
        call before failing the entire 'send' call. Corresponds to Kafka's 'max.block.ms'
        property}
    zh: {description: 在“send”调用失败之前，发布者将等待获取元数据或等待缓冲区刷新的时间。对应于Kafka的'max.block。ms'属性,
      displayName: 最大元数据等待时间}
  message-header-encoding:
    en: {displayName: Message Header Encoding, description: 'For any attribute that
        is added as a message header, as configured via the <Attributes to Send as
        Headers> property, this property indicates the Character Encoding to use for
        serializing the headers.'}
    zh: {description: 对于通过<Attributes to Send as Headers>属性配置的作为消息头添加的任何属性，此属性指示用于序列化头的字符编码。,
      displayName: 消息头编码}
  kerberos-user-service:
    en: {displayName: Kerberos User Service, description: Service supporting user
        authentication with Kerberos}
    zh: {description: 使用Kerberos支持用户身份验证的服务, displayName: Kerberos用户服务}
  message-demarcator:
    en: {displayName: Message Demarcator, description: 'Specifies the string (interpreted
        as UTF-8) to use for demarcating multiple messages within a single FlowFile.
        If not specified, the entire content of the FlowFile will be used as a single
        message. If specified, the contents of the FlowFile will be split on this
        delimiter and each section sent as a separate Kafka message. To enter special
        character such as ''new line'' use CTRL+Enter or Shift+Enter, depending on
        your OS.'}
    zh: {description: 指定用于在单个FlowFile中划分多个消息的字符串（解释为UTF-8）。如果未指定，FlowFile的全部内容将作为单个消息使用。如果指定，FlowFile的内容将在该分隔符上拆分，每个部分将作为单独的Kafka消息发送。要输入特殊字符，如“换行”，请使用CTRL+enter或Shift+enter，具体取决于您的操作系统。,
      displayName: 消息标定器}
  transactional-id-prefix:
    en: {displayName: Transactional Id Prefix, description: 'When Use Transaction
        is set to true, KafkaProducer config ''transactional.id'' will be a generated
        UUID and will be prefixed with this string.'}
    zh: {description: 当Use Transaction设置为true时，KafkaProducer将配置“transactional”。id'将是生成的UUID，并将以该字符串作为前缀。,
      displayName: 事务Id前缀}
  ack.wait.time:
    en: {displayName: Acknowledgment Wait Time, description: 'After sending a message
        to Kafka, this indicates the amount of time that we are willing to wait for
        a response from Kafka. If Kafka does not acknowledge the message within this
        time period, the FlowFile will be routed to ''failure''.'}
    zh: {description: 在向卡夫卡发送消息后，这表明我们愿意等待卡夫卡回复的时间。如果Kafka在这段时间内没有确认消息，FlowFile将被路由到“失败”。,
      displayName: 确认等待时间}
  sasl.username:
    en: {displayName: Username, description: Username provided with configured password
        when using PLAIN or SCRAM SASL Mechanisms}
    zh: {description: 使用PLAIN或SCRAM SASL机制时提供的用户名和配置的密码, displayName: 用户名}
  use-transactions:
    en: {displayName: Use Transactions, description: 'Specifies whether or not NiFi
        should provide Transactional guarantees when communicating with Kafka. If
        there is a problem sending data to Kafka, and this property is set to false,
        then the messages that have already been sent to Kafka will continue on and
        be delivered to consumers. If this is set to true, then the Kafka transaction
        will be rolled back so that those messages are not available to consumers.
        Setting this to true requires that the <Delivery Guarantee> property be set
        to "Guarantee Replicated Delivery."'}
    zh: {description: 指定NiFi在与Kafka通信时是否应提供事务性保证。如果向Kafka发送数据时出现问题，并且该属性设置为false，那么已经发送给Kafka的消息将继续发送并传递给消费者。如果设置为true，那么Kafka事务将被回滚，以便消费者无法使用这些消息。将此设置为true需要将<Delivery
        Guaranty>属性设置为“Guaranty Replicated Delivery”, displayName: 使用事务处理}
  acks:
    en: {displayName: Delivery Guarantee, description: Specifies the requirement for
        guaranteeing that a message is sent to Kafka. Corresponds to Kafka's 'acks'
        property.}
    zh: {description: 指定确保向Kafka发送消息的要求。对应于卡夫卡的“acks”属性。, displayName: 交货保证书}
  aws.profile.name:
    en: {displayName: AWS Profile Name, description: The Amazon Web Services Profile
        to select when multiple profiles are available.}
    zh: {description: 当多个配置文件可用时，选择Amazon Web Services配置文件。, displayName: AWS配置文件名称}
  security.protocol:
    en: {displayName: Security Protocol, description: Security protocol used to communicate
        with brokers. Corresponds to Kafka Client security.protocol property}
    zh: {description: 用于与经纪人通信的安全协议。对应于Kafka客户端安全。协议属性, displayName: 安全协议}
  ssl.context.service:
    en: {displayName: SSL Context Service, description: Service supporting SSL communication
        with Kafka brokers}
    zh: {description: 支持与Kafka代理进行SSL通信的服务, displayName: SSL上下文服务}
  max.request.size:
    en: {displayName: Max Request Size, description: The maximum size of a request
        in bytes. Corresponds to Kafka's 'max.request.size' property and defaults
        to 1 MB (1048576).}
    zh: {description: 请求的最大大小（字节）。对应于卡夫卡的max.request。size”属性，默认值为1 MB（1048576）。, displayName: 最大请求大小}
  sasl.token.auth:
    en: {displayName: Token Authentication, description: Enables or disables Token
        authentication when using SCRAM SASL Mechanisms}
    zh: {description: 使用SCRAM SASL机制时启用或禁用令牌身份验证, displayName: 令牌身份验证}
  Failure Strategy:
    en: {displayName: Failure Strategy, description: Specifies how the processor handles
        a FlowFile if it is unable to publish the data to Kafka}
    zh: {description: 指定处理器无法将数据发布到Kafka时如何处理FlowFile, displayName: 故障策略}
  partitioner.class:
    en: {displayName: Partitioner class, description: Specifies which class to use
        to compute a partition id for a message. Corresponds to Kafka's 'partitioner.class'
        property.}
    zh: {description: 指定用于计算消息的分区id的类。对应于卡夫卡的《分裂者》。类的属性。, displayName: 分区器类}
  sasl.kerberos.keytab:
    en: {displayName: Kerberos Keytab, description: Keytab credentials used for authentication
        with Kerberos}
    zh: {description: 用于Kerberos身份验证的Keytab凭据, displayName: Kerberos密钥选项卡}
  topic:
    en: {displayName: Topic Name, description: The name of the Kafka Topic to publish
        to.}
    zh: {description: 要发布到的Kafka主题的名称。, displayName: 主题名称}
  sasl.password:
    en: {displayName: Password, description: Password provided with configured username
        when using PLAIN or SCRAM SASL Mechanisms}
    zh: {description: 使用PLAIN或SCRAM SASL机制时，密码随配置的用户名一起提供, displayName: 暗语}
  key-attribute-encoding:
    en: {displayName: Key Attribute Encoding, description: FlowFiles that are emitted
        have an attribute named 'kafka.key'. This property dictates how the value
        of the attribute should be encoded.}
    zh: {description: 发出的FlowFiles具有名为“kafka.key”的属性。此属性指定属性值的编码方式。, displayName: 关键属性编码}
writeAttributes:
  msg.count: {en: 'The number of messages that were sent to Kafka for this FlowFile.
      This attribute is added only to FlowFiles that are routed to success. If the
      <Message Demarcator> Property is not set, this will always be 1, but if the
      Property is set, it may be greater than 1.', zh: 为此FlowFile发送给Kafka的消息数。此属性仅添加到路由成功的FlowFiles。如果未设置＜消息分界符＞属性，该值将始终为1，但如果设置了属性，则该值可能大于1。}
tags:
  en: [Apache, Kafka, Put, Send, Message, PubSub, '2.6']
  zh: [阿帕奇, 卡夫卡, 放, 邮寄, 消息, 发布Sub, '2.6']
