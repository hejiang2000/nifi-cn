relationships:
  success: {en: All FlowFiles are transferred to this relationship, zh: 所有FlowFiles都传输到此关系}
capabilityDescription: {en: 'Retrieves a listing of files from HDFS. Each time a listing
    is performed, the files with the latest timestamp will be excluded and picked
    up during the next execution of the processor. This is done to ensure that we
    do not miss any files, or produce duplicates, in the cases where files with the
    same timestamp are written immediately before and after a single execution of
    the processor. For each file that is listed in HDFS, this processor creates a
    FlowFile that represents the HDFS file to be fetched in conjunction with FetchHDFS.
    This Processor is designed to run on Primary Node only in a cluster. If the primary
    node changes, the new Primary Node will pick up where the previous node left off
    without duplicating all of the data. Unlike GetHDFS, this Processor does not delete
    any data from HDFS.', zh: 从HDFS中检索文件列表。每次执行列表时，具有最新时间戳的文件将被排除在外，并在处理器的下一次执行期间被拾取。这样做是为了确保在处理器执行一次之前和之后立即写入具有相同时间戳的文件的情况下，不会丢失任何文件或产生重复文件。对于HDFS中列出的每个文件，该处理器创建一个FlowFile，该FlowFile表示要与FetchHDFS一起获取的HDFS文件。此处理器设计为仅在群集中的主节点上运行。如果主节点发生更改，新的主节点将在不复制所有数据的情况下恢复上一个节点停止的位置。与GetHDFS不同，此处理器不会从HDFS中删除任何数据。}
statefulDescription: {en: 'After performing a listing of HDFS files, the latest timestamp
    of all the files listed and the latest timestamp of all the files transferred
    are both stored. This allows the Processor to list only files that have been added
    or modified after this date the next time that the Processor is run, without having
    to store all of the actual filenames/paths which could lead to performance problems.
    State is stored across the cluster so that this Processor can be run on Primary
    Node only and if a new Primary Node is selected, the new node can pick up where
    the previous node left off, without duplicating the data.', zh: 执行HDFS文件列表后，将同时存储列出的所有文件的最新时间戳和传输的所有文件最新时间戳记。这允许处理器在下次运行处理器时仅列出在此日期之后添加或修改的文件，而不必存储可能导致性能问题的所有实际文件名/路径。状态存储在整个集群中，因此此处理器只能在主节点上运行，如果选择了新的主节点，则新节点可以从上一个节点停止的位置恢复，而无需复制数据。}
properties:
  Hadoop Configuration Resources:
    en: {displayName: Hadoop Configuration Resources, description: 'A file or comma
        separated list of files which contains the Hadoop file system configuration.
        Without this, Hadoop will search the classpath for a ''core-site.xml'' and
        ''hdfs-site.xml'' file or will revert to a default configuration. To use swebhdfs,
        see ''Additional Details'' section of PutHDFS''s documentation.'}
    zh: {description: 包含Hadoop文件系统配置的文件或逗号分隔的文件列表。否则，Hadoop将在类路径中搜索“核心站点”。xml'和'hdfs站点。xml文件，或将恢复为默认配置。要使用swebhdfs，请参阅PutHDFS文档的“其他详细信息”部分。,
      displayName: Hadoop配置资源}
  maximum-file-age:
    en: {displayName: Maximum File Age, description: The maximum age that a file must
        be in order to be pulled; any file older than this amount of time (based on
        last modification date) will be ignored. Minimum value is 100ms.}
    zh: {description: 文件必须达到的最长使用期限才能被提取；任何早于此时间（基于上次修改日期）的文件都将被忽略。最小值为100ms。, displayName: 最大文件期限}
  record-writer:
    en: {displayName: Record Writer, description: 'Specifies the Record Writer to
        use for creating the listing. If not specified, one FlowFile will be created
        for each entity that is listed. If the Record Writer is specified, all entities
        will be written to a single FlowFile.'}
    zh: {description: 指定用于创建列表的记录编写器。如果未指定，将为列出的每个实体创建一个FlowFile。如果指定了Record Writer，则所有实体都将写入单个FlowFile。,
      displayName: 记录编写器}
  Additional Classpath Resources:
    en: {displayName: Additional Classpath Resources, description: 'A comma-separated
        list of paths to files and/or directories that will be added to the classpath
        and used for loading native libraries. When specifying a directory, all files
        with in the directory will be added to the classpath, but further sub-directories
        will not be included.'}
    zh: {description: 以逗号分隔的文件和/或目录路径列表，这些路径将添加到类路径中并用于加载本机库。指定目录时，目录中的所有文件都将添加到类路径中，但不包括其他子目录。,
      displayName: 其他Classpath资源}
  Distributed Cache Service:
    en: {displayName: Distributed Cache Service, description: This property is ignored.  State
        will be stored in the LOCAL or CLUSTER scope by the State Manager based on
        NiFi's configuration.}
    zh: {description: 此属性被忽略。状态管理器将根据NiFi的配置将状态存储在LOCAL或CLUSTER范围中。, displayName: 分布式缓存服务}
  Recurse Subdirectories:
    en: {displayName: Recurse Subdirectories, description: Indicates whether to list
        files from subdirectories of the HDFS directory}
    zh: {description: 指示是否从HDFS目录的子目录中列出文件, displayName: 定期子目录}
  file-filter-mode:
    en: {displayName: File Filter Mode, description: Determines how the regular expression
        in  File Filter will be used when retrieving listings.}
    zh: {description: 确定检索列表时如何使用文件筛选器中的正则表达式。, displayName: 文件筛选器模式}
  File Filter:
    en: {displayName: File Filter, description: Only files whose names match the given
        regular expression will be picked up}
    zh: {description: 只有名称与给定正则表达式匹配的文件才会被拾取, displayName: 文件筛选器}
  kerberos-credentials-service:
    en: {displayName: Kerberos Credentials Service, description: Specifies the Kerberos
        Credentials Controller Service that should be used for authenticating with
        Kerberos}
    zh: {description: 指定应用于Kerberos身份验证的Kerberos凭据控制器服务, displayName: Kerberos凭据服务}
  Kerberos Password:
    en: {displayName: Kerberos Password, description: Kerberos password associated
        with the principal.}
    zh: {description: 与主体关联的Kerberos密码。, displayName: Kerberos密码}
  Kerberos Keytab:
    en: {displayName: Kerberos Keytab, description: Kerberos keytab associated with
        the principal. Requires nifi.kerberos.krb5.file to be set in your nifi.properties}
    zh: {description: 与主体关联的Kerberos密钥表。需要在nifi.properties中设置nifi.krberos.krb5.file,
      displayName: Kerberos密钥选项卡}
  Kerberos Principal:
    en: {displayName: Kerberos Principal, description: Kerberos principal to authenticate
        as. Requires nifi.kerberos.krb5.file to be set in your nifi.properties}
    zh: {description: 要作为身份验证的Kerberos主体。需要在nifi.properties中设置nifi.krberos.krb5.file,
      displayName: Kerberos主体}
  kerberos-user-service:
    en: {displayName: Kerberos User Service, description: Specifies the Kerberos User
        Controller Service that should be used for authenticating with Kerberos}
    zh: {description: 指定应用于Kerberos身份验证的Kerberos用户控制器服务, displayName: Kerberos用户服务}
  Kerberos Relogin Period:
    en:
      displayName: Kerberos Relogin Period
      description: |-
        Period of time which should pass before attempting a kerberos relogin.

        This property has been deprecated, and has no effect on processing. Relogins now occur automatically.
    zh: {description: 在尝试kerberos重新登录之前应该经过的时间段。, displayName: Kerberos时钟周期}
  minimum-file-age:
    en: {displayName: Minimum File Age, description: The minimum age that a file must
        be in order to be pulled; any file younger than this amount of time (based
        on last modification date) will be ignored}
    zh: {description: 文件必须达到的最短使用期限才能被提取；任何小于此时间（基于上次修改日期）的文件都将被忽略, displayName: 最小文件期限}
  Directory:
    en: {displayName: Directory, description: The HDFS directory from which files
        should be read}
    zh: {description: 应从中读取文件的HDFS目录, displayName: 目录}
writeAttributes:
  path: {en: 'The path is set to the absolute path of the file''s directory on HDFS.
      For example, if the Directory property is set to /tmp, then files picked up
      from /tmp will have the path attribute set to "./". If the Recurse Subdirectories
      property is set to true and a file is picked up from /tmp/abc/1/2/3, then the
      path attribute will be set to "/tmp/abc/1/2/3".', zh: 路径设置为HDFS上文件目录的绝对路径。例如，如果Directory属性设置为/tmp，则从/tmp拾取的文件的路径属性将设置为“./”。如果Recurse
      Subdirectories属性设置为true，并且从/tmp/abc/1/2/3获取文件，则路径属性将设置为“/tmp/abs/1/2/3”。}
  filename: {en: The name of the file that was read from HDFS., zh: 从HDFS读取的文件的名称。}
  hdfs.owner: {en: The user that owns the file in HDFS, zh: HDFS中拥有文件的用户}
  hdfs.length: {en: The number of bytes in the file in HDFS, zh: HDFS中文件的字节数}
  hdfs.lastModified: {en: 'The timestamp of when the file in HDFS was last modified,
      as milliseconds since midnight Jan 1, 1970 UTC', zh: HDFS中文件上次修改的时间戳，以UTC 1970年1月1日午夜后的毫秒为单位}
  hdfs.replication: {en: The number of HDFS replicas for hte file, zh: 该文件的HDFS副本数}
  hdfs.group: {en: The group that owns the file in HDFS, zh: HDFS中拥有文件的组}
  hdfs.permissions: {en: 'The permissions for the file in HDFS. This is formatted
      as 3 characters for the owner, 3 for the group, and 3 for other users. For example
      rw-rw-r--', zh: HDFS中文件的权限。所有者的格式为3个字符，组为3个，其他用户为3个。例如rw-rw-r--}
tags:
  en: [hadoop, HCFS, HDFS, get, list, ingest, source, filesystem]
  zh: [hadoop公司, 人类基因组学, 高密度光纤, 收到, 列表, 摄入, 来源, 文件系统]
