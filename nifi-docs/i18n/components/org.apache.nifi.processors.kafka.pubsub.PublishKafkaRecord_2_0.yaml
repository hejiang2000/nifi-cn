relationships:
  success: {en: FlowFiles for which all content was sent to Kafka., zh: 所有内容都发送到Kafka的FlowFiles。}
  failure: {en: Any FlowFile that cannot be sent to Kafka will be routed to this Relationship,
    zh: 无法发送到Kafka的任何FlowFile都将被路由到此关系}
dynamicProperties:
  The name of a Kafka configuration property.:
    en: {description: 'These properties will be added on the Kafka configuration after
        loading any provided configuration properties. In the event a dynamic property
        represents a property that was already set, its value will be ignored and
        WARN message logged. For the list of available Kafka properties please refer
        to: http://kafka.apache.org/documentation.html#configuration. ', value: The
        value of a given Kafka configuration property.}
    zh: {description: '这些属性将在加载任何提供的配置属性后添加到Kafka配置中。如果动态属性表示已设置的属性，则将忽略其值并记录WARN消息。有关可用Kafka属性的列表，请参阅：http://kafka.apache.org/documentation.html#configuration.',
      value: 给定Kafka配置属性的值。}
capabilityDescription: {en: Sends the contents of a FlowFile as individual records
    to Apache Kafka using the Kafka 2.0 Producer API. The contents of the FlowFile
    are expected to be record-oriented data that can be read by the configured Record
    Reader. The complementary NiFi processor for fetching messages is ConsumeKafkaRecord_2_0.,
  zh: 使用Kafka 2.0 Producer API将FlowFile的内容作为单个记录发送到Apache Kafka。FlowFile的内容应为可由配置的记录读取器读取的面向记录的数据。用于获取消息的补充NiFi处理器是ConsumerKafkaRecord_2_0。}
properties:
  compression.type:
    en: {displayName: Compression Type, description: This parameter allows you to
        specify the compression codec for all data generated by this producer.}
    zh: {description: 此参数允许您为此生成器生成的所有数据指定压缩编解码器。, displayName: 压缩类型}
  attribute-name-regex:
    en: {displayName: Attributes to Send as Headers (Regex), description: 'A Regular
        Expression that is matched against all FlowFile attribute names. Any attribute
        whose name matches the regex will be added to the Kafka messages as a Header.
        If not specified, no FlowFile attributes will be added as headers.'}
    zh: {description: 与所有FlowFile属性名称匹配的正则表达式。任何名称与正则表达式匹配的属性都将作为Header添加到Kafka消息中。如果未指定，则不会将FlowFile属性添加为标头。,
      displayName: 要作为标头发送的属性（Regex）}
  record-writer:
    en: {displayName: Record Writer, description: The Record Writer to use in order
        to serialize the data before sending to Kafka}
    zh: {description: 用于在发送到Kafka之前序列化数据的Record Writer, displayName: 记录编写器}
  bootstrap.servers:
    en: {displayName: Kafka Brokers, description: 'Comma-separated list of Kafka Brokers
        in the format host:port'}
    zh: {description: 'Kafka Broker的逗号分隔列表，格式为host:port', displayName: 卡夫卡经纪人}
  sasl.kerberos.principal:
    en: {displayName: Kerberos Principal, description: Principal used for authentication
        with Kerberos}
    zh: {description: 用于Kerberos身份验证的主体, displayName: Kerberos主体}
  sasl.kerberos.service.name:
    en: {displayName: Kerberos Service Name, description: The service name that matches
        the primary name of the Kafka server configured in the broker JAAS configuration}
    zh: {description: 与代理JAAS配置中配置的Kafka服务器的主名称匹配的服务名称, displayName: Kerberos服务名称}
  kerberos-credentials-service:
    en: {displayName: Kerberos Credentials Service, description: Service supporting
        generalized credentials authentication with Kerberos}
    zh: {description: 支持Kerberos通用凭据身份验证的服务, displayName: Kerberos凭据服务}
  partition:
    en: {displayName: Partition, description: Specifies which Partition Records will
        go to. How this value is interpreted is dictated by the <Partitioner class>
        property.}
    zh: {description: 指定将转到哪个分区记录。此值的解释方式由<Partitioner class>属性决定。, displayName: 隔断}
  sasl.mechanism:
    en: {displayName: SASL Mechanism, description: SASL mechanism used for authentication.
        Corresponds to Kafka Client sasl.mechanism property}
    zh: {description: 用于身份验证的SASL机制。对应Kafka Client sasl。机械特性, displayName: SASL机制}
  max.block.ms:
    en: {displayName: Max Metadata Wait Time, description: The amount of time publisher
        will wait to obtain metadata or wait for the buffer to flush during the 'send'
        call before failing the entire 'send' call. Corresponds to Kafka's 'max.block.ms'
        property}
    zh: {description: 在“send”调用失败之前，发布者将等待获取元数据或等待缓冲区刷新的时间。对应于Kafka的'max.block。ms'属性,
      displayName: 最大元数据等待时间}
  record-reader:
    en: {displayName: Record Reader, description: The Record Reader to use for incoming
        FlowFiles}
    zh: {description: 用于传入FlowFiles的记录读取器, displayName: 记录读取器}
  message-header-encoding:
    en: {displayName: Message Header Encoding, description: 'For any attribute that
        is added as a message header, as configured via the <Attributes to Send as
        Headers> property, this property indicates the Character Encoding to use for
        serializing the headers.'}
    zh: {description: 对于通过<Attributes to Send as Headers>属性配置的作为消息头添加的任何属性，此属性指示用于序列化头的字符编码。,
      displayName: 消息头编码}
  transactional-id-prefix:
    en: {displayName: Transactional Id Prefix, description: 'When Use Transaction
        is set to true, KafkaProducer config ''transactional.id'' will be a generated
        UUID and will be prefixed with this string.'}
    zh: {description: 当Use Transaction设置为true时，KafkaProducer将配置“transactional”。id'将是生成的UUID，并将以该字符串作为前缀。,
      displayName: 事务Id前缀}
  ack.wait.time:
    en: {displayName: Acknowledgment Wait Time, description: 'After sending a message
        to Kafka, this indicates the amount of time that we are willing to wait for
        a response from Kafka. If Kafka does not acknowledge the message within this
        time period, the FlowFile will be routed to ''failure''.'}
    zh: {description: 在向卡夫卡发送消息后，这表明我们愿意等待卡夫卡回复的时间。如果Kafka在这段时间内没有确认消息，FlowFile将被路由到“失败”。,
      displayName: 确认等待时间}
  sasl.username:
    en: {displayName: Username, description: Username provided with configured password
        when using PLAIN or SCRAM SASL Mechanisms}
    zh: {description: 使用PLAIN或SCRAM SASL机制时提供的用户名和配置的密码, displayName: 用户名}
  use-transactions:
    en: {displayName: Use Transactions, description: 'Specifies whether or not NiFi
        should provide Transactional guarantees when communicating with Kafka. If
        there is a problem sending data to Kafka, and this property is set to false,
        then the messages that have already been sent to Kafka will continue on and
        be delivered to consumers. If this is set to true, then the Kafka transaction
        will be rolled back so that those messages are not available to consumers.
        Setting this to true requires that the <Delivery Guarantee> property be set
        to "Guarantee Replicated Delivery."'}
    zh: {description: 指定NiFi在与Kafka通信时是否应提供事务性保证。如果向Kafka发送数据时出现问题，并且该属性设置为false，那么已经发送给Kafka的消息将继续发送并传递给消费者。如果设置为true，那么Kafka事务将被回滚，以便消费者无法使用这些消息。将此设置为true需要将<Delivery
        Guaranty>属性设置为“Guaranty Replicated Delivery”, displayName: 使用事务处理}
  acks:
    en: {displayName: Delivery Guarantee, description: Specifies the requirement for
        guaranteeing that a message is sent to Kafka. Corresponds to Kafka's 'acks'
        property.}
    zh: {description: 指定确保向Kafka发送消息的要求。对应于卡夫卡的“acks”属性。, displayName: 交货保证书}
  security.protocol:
    en: {displayName: Security Protocol, description: Security protocol used to communicate
        with brokers. Corresponds to Kafka Client security.protocol property}
    zh: {description: 用于与经纪人通信的安全协议。对应于Kafka客户端安全。协议属性, displayName: 安全协议}
  ssl.context.service:
    en: {displayName: SSL Context Service, description: Service supporting SSL communication
        with Kafka brokers}
    zh: {description: 支持与Kafka代理进行SSL通信的服务, displayName: SSL上下文服务}
  max.request.size:
    en: {displayName: Max Request Size, description: The maximum size of a request
        in bytes. Corresponds to Kafka's 'max.request.size' property and defaults
        to 1 MB (1048576).}
    zh: {description: 请求的最大大小（字节）。对应于卡夫卡的max.request。size”属性，默认值为1 MB（1048576）。, displayName: 最大请求大小}
  sasl.token.auth:
    en: {displayName: Token Authentication, description: Enables or disables Token
        authentication when using SCRAM SASL Mechanisms}
    zh: {description: 使用SCRAM SASL机制时启用或禁用令牌身份验证, displayName: 令牌身份验证}
  message-key-field:
    en: {displayName: Message Key Field, description: The name of a field in the Input
        Records that should be used as the Key for the Kafka message.}
    zh: {description: 输入记录中应用作Kafka消息密钥的字段的名称。, displayName: 消息密钥字段}
  Failure Strategy:
    en: {displayName: Failure Strategy, description: Specifies how the processor handles
        a FlowFile if it is unable to publish the data to Kafka}
    zh: {description: 指定处理器无法将数据发布到Kafka时如何处理FlowFile, displayName: 故障策略}
  partitioner.class:
    en: {displayName: Partitioner class, description: Specifies which class to use
        to compute a partition id for a message. Corresponds to Kafka's 'partitioner.class'
        property.}
    zh: {description: 指定用于计算消息的分区id的类。对应于卡夫卡的《分裂者》。类的属性。, displayName: 分区器类}
  sasl.kerberos.keytab:
    en: {displayName: Kerberos Keytab, description: Keytab credentials used for authentication
        with Kerberos}
    zh: {description: 用于Kerberos身份验证的Keytab凭据, displayName: Kerberos密钥选项卡}
  topic:
    en: {displayName: Topic Name, description: The name of the Kafka Topic to publish
        to.}
    zh: {description: 要发布到的Kafka主题的名称。, displayName: 主题名称}
  sasl.password:
    en: {displayName: Password, description: Password provided with configured username
        when using PLAIN or SCRAM SASL Mechanisms}
    zh: {description: 使用PLAIN或SCRAM SASL机制时，密码随配置的用户名一起提供, displayName: 暗语}
writeAttributes:
  msg.count: {en: The number of messages that were sent to Kafka for this FlowFile.
      This attribute is added only to FlowFiles that are routed to success., zh: 为此FlowFile发送给Kafka的消息数。此属性仅添加到路由成功的FlowFiles。}
tags:
  en: [Apache, Kafka, Record, csv, json, avro, logs, Put, Send, Message, PubSub, '2.0']
  zh: [阿帕奇, 卡夫卡, 记录, csv格式, json文件, 我没有吗？, 日志, 放, 邮寄, 消息, 发布Sub, '2.0']
